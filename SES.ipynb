{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional\n",
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "from b2aiprep.dataset import VBAIDataset\n",
    "from b2aiprep.process import Audio, specgram, plot_spectrogram\n",
    "import IPython.display as Ipd\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audiorecordings(from lectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VBAIDataset('../bids_with_sensitive_recordings')\n",
    "# data = torch.load('./bids_with_sensitive_recordings/sub-0e2df8b3-a93f-4982-a82c-d96a5c64d153/ses-461EA3E8-4477-4F97-B091-D21F4006B2FC/audio/sub-0e2df8b3-a93f-4982-a82c-d96a5c64d153_ses-461EA3E8-4477-4F97-B091-D21F4006B2FC_Audio-Check_rec-Audio-Check-1.pt')\n",
    "# print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 142\n",
      "val: 18\n",
      "test: 18\n"
     ]
    }
   ],
   "source": [
    "participant_df = dataset.load_and_pivot_questionnaire('participant')\n",
    "all_identities = sorted(participant_df['record_id'].to_numpy().tolist())\n",
    "\n",
    "N = len(all_identities)\n",
    "\n",
    "train_identities = set(all_identities[:int(0.8*N)])\n",
    "val_identities = set(all_identities[int(0.8*N):int(0.9*N)])\n",
    "test_identities = set(all_identities[int(0.9*N):])\n",
    "\n",
    "print('train:', len(train_identities))\n",
    "print('val:', len(val_identities))\n",
    "print('test:', len(test_identities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 204 person/session pairs\n"
     ]
    }
   ],
   "source": [
    "qs = dataset.load_questionnaires('recordingschema')\n",
    "q_dfs = []\n",
    "for i, questionnaire in enumerate(qs):\n",
    "    df = dataset.questionnaire_to_dataframe(questionnaire)\n",
    "    df['dataframe_number'] = i\n",
    "    q_dfs.append(df)\n",
    "    i += 1\n",
    "recordingschema_df = pd.concat(q_dfs)\n",
    "recordingschema_df = pd.pivot(recordingschema_df, index='dataframe_number', columns='linkId', values='valueString')\n",
    "\n",
    "person_session_pairs = recordingschema_df[['record_id', 'recording_session_id']].to_numpy().astype(str)\n",
    "person_session_pairs = np.unique(person_session_pairs, axis=0).tolist()\n",
    "\n",
    "print('Found {} person/session pairs'.format(len(person_session_pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAudioDataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, identities, dataset, person_session_pairs, segment_size=3):\n",
    "\t\tself.segment_size = segment_size\n",
    "\t\t\n",
    "\t\t# get age and airway stenosis classification for all subjects\n",
    "\t\tparticipant_df = dataset.load_and_pivot_questionnaire('participant')\n",
    "\t\tage_df = participant_df[['record_id', 'age']].to_numpy()\n",
    "\t\tairway_stenosis_df = participant_df[['record_id', 'airway_stenosis']].to_numpy()\n",
    "        \n",
    "\t\tage_dict = {}\n",
    "\t\tfor person_id, age in age_df:\n",
    "\t\t\tage_dict[str(person_id)] = float(age)\n",
    "\t\tairway_stenosis_dict = {}\n",
    "\t\tfor person_id, airway_stenosis in airway_stenosis_df:\n",
    "\t\t\tairway_stenosis_dict[str(person_id)] = float(airway_stenosis)\n",
    "\n",
    "\t\t# get all prolonged vowel audios\n",
    "\t\tself.audio_files = []\n",
    "\t\tself.age = []\n",
    "\t\tself.airway_stenosis = []\n",
    "        \n",
    "\t\tfor person_id, session_id in person_session_pairs:\n",
    "\t\t\tif person_id not in identities:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tvowel_audios = [str(x) for x in dataset.find_audio(person_id, session_id) if str(x).endswith('-Prolonged-vowel.wav')]\n",
    "\t\t\tself.audio_files += vowel_audios\n",
    "\t\t\tself.age += [age_dict[person_id]]*len(vowel_audios)\n",
    "\t\t\tself.airway_stenosis += [airway_stenosis_dict[person_id]]*len(vowel_audios)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.audio_files)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\taudio = Audio.from_file(self.audio_files[idx])\n",
    "\t\taudio = audio.to_16khz().signal.squeeze()\n",
    "\t\t# get middle K seconds if audio is too long, pad with zeros if it is too short\n",
    "\t\tif audio.size(0) > self.segment_size*16000:\n",
    "\t\t\td = (audio.size(0)-self.segment_size*16000)//2\n",
    "\t\t\taudio = audio[d:d+self.segment_size*16000]\n",
    "\t\telse:\n",
    "\t\t\taudio = torch.nn.functional.pad(audio, (0,self.segment_size*16000-audio.size(0)), mode='constant', value=0)\n",
    "\t\treturn {'signal': audio, 'age': self.age[idx], 'airway_stenosis': self.airway_stenosis[idx]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>record_id</th>\n",
       "      <th>demographics_session_id</th>\n",
       "      <th>demographics_duration</th>\n",
       "      <th>demographics_completed_by___1</th>\n",
       "      <th>demographics_completed_by___2</th>\n",
       "      <th>demographics_completed_by___3</th>\n",
       "      <th>state_province</th>\n",
       "      <th>country</th>\n",
       "      <th>gender_identity</th>\n",
       "      <th>...</th>\n",
       "      <th>household_count</th>\n",
       "      <th>spouse_partner_sig_other</th>\n",
       "      <th>children</th>\n",
       "      <th>parent</th>\n",
       "      <th>grandparent</th>\n",
       "      <th>other_live_with</th>\n",
       "      <th>others_household_specify</th>\n",
       "      <th>transportation_yn</th>\n",
       "      <th>primary_transportation</th>\n",
       "      <th>q_generic_demographics_complete</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>8d5dc52b-e8aa-42e7-ae54-8f05c4667d39</td>\n",
       "      <td>B176636C-3330-4AB4-93A9-1E2305506407</td>\n",
       "      <td>173.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>USA</td>\n",
       "      <td>Female gender identity</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Personal vehicle</td>\n",
       "      <td>Complete</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                             record_id  \\\n",
       "0           0  8d5dc52b-e8aa-42e7-ae54-8f05c4667d39   \n",
       "\n",
       "                demographics_session_id  demographics_duration  \\\n",
       "0  B176636C-3330-4AB4-93A9-1E2305506407                  173.0   \n",
       "\n",
       "   demographics_completed_by___1  demographics_completed_by___2  \\\n",
       "0                           True                          False   \n",
       "\n",
       "   demographics_completed_by___3 state_province country  \\\n",
       "0                          False      Tennessee     USA   \n",
       "\n",
       "          gender_identity  ... household_count spouse_partner_sig_other  \\\n",
       "0  Female gender identity  ...             4.0                       No   \n",
       "\n",
       "   children  parent  grandparent  other_live_with  others_household_specify  \\\n",
       "0       Yes     Yes           No               No                       NaN   \n",
       "\n",
       "   transportation_yn  primary_transportation  q_generic_demographics_complete  \n",
       "0                Yes        Personal vehicle                         Complete  \n",
       "\n",
       "[1 rows x 57 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg = pd.read_csv('../demographics.csv')\n",
    "dg.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>demographics_session_id</th>\n",
       "      <th>household_income_usa</th>\n",
       "      <th>household_income_ca</th>\n",
       "      <th>household_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8d5dc52b-e8aa-42e7-ae54-8f05c4667d39</td>\n",
       "      <td>B176636C-3330-4AB4-93A9-1E2305506407</td>\n",
       "      <td>$15,000 to $29,999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1b07b18b-26f9-405b-a466-29442306a7fe</td>\n",
       "      <td>8F8E68BB-E68C-4EA5-B71A-17D7AAE915C2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$150,000 to $199,999</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e5db3e0c-6589-4a15-a5e7-8a95e4ed34a5</td>\n",
       "      <td>B94FE4BC-79FF-46A1-86CC-628E2D77874E</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$50,000 to $99,999</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              record_id               demographics_session_id  \\\n",
       "0  8d5dc52b-e8aa-42e7-ae54-8f05c4667d39  B176636C-3330-4AB4-93A9-1E2305506407   \n",
       "1  1b07b18b-26f9-405b-a466-29442306a7fe  8F8E68BB-E68C-4EA5-B71A-17D7AAE915C2   \n",
       "2  e5db3e0c-6589-4a15-a5e7-8a95e4ed34a5  B94FE4BC-79FF-46A1-86CC-628E2D77874E   \n",
       "\n",
       "  household_income_usa   household_income_ca  household_count  \n",
       "0   $15,000 to $29,999                   NaN              4.0  \n",
       "1                  NaN  $150,000 to $199,999              4.0  \n",
       "2                  NaN    $50,000 to $99,999              1.0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = dg[[\"record_id\", \"demographics_session_id\"]]\n",
    "Y_income = dg [[\"household_income_usa\", \"household_income_ca\", \"household_count\"]]\n",
    "\n",
    "Train = dg[[\"record_id\", \"demographics_session_id\", \"household_income_usa\", \"household_income_ca\", \"household_count\"]]\n",
    "Train.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before filtering:  (179, 3)\n",
      "Shape after filtering:  (115, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape before filtering: \",Y_income.shape)\n",
    "pre_Y_train = Y_income[(pd.notna(Y_income['household_income_usa']) | pd.notna(Y_income['household_income_ca'])) &\n",
    "                                   (~((Y_income['household_income_usa'] == 'Prefer not to answer') | \n",
    "                                      (Y_income['household_income_ca'] == 'Prefer not to answer')))]\n",
    "print(\"Shape after filtering: \",pre_Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded labels:  tensor([[1, 0, 0, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 0, 1, 0]])\n",
      "Shape:  torch.Size([115, 4])\n"
     ]
    }
   ],
   "source": [
    "#label data: 0 - poverty, 1 - lower, 2 - middle, 3 - upper\n",
    "Y_train = pd.DataFrame()\n",
    "\n",
    "for index, row in pre_Y_train.iterrows():\n",
    "    if pd.notna(pre_Y_train.loc[index, \"household_income_usa\"]) and pre_Y_train.loc[index, \"household_count\"] >= 3: # USD; HH >= 3\n",
    "        income = pre_Y_train.loc[index, \"household_income_usa\"]\n",
    "        if income in ['< $15,000', '$15,000 to $29,999']:\n",
    "            Y_train.at[index, \"SES\"] = 0\n",
    "        elif income in ['$30,000 to $$49,999']:\n",
    "            Y_train.at[index, \"SES\"] = 1\n",
    "        elif income in ['$50,000 to $99,999', '$100,000 to $149,999', '$150,000 to $199,999']:\n",
    "            Y_train.at[index, \"SES\"] = 2\n",
    "        elif income in ['$200,000 to $249,999', '>$250,000']:\n",
    "            Y_train.at[index, \"SES\"] = 3\n",
    "        elif income in ['Prefer not to answer']:\n",
    "            continue\n",
    "        else:\n",
    "            print(income)\n",
    "            raise ValueError(\"Wrong value for household_income_usa\")\n",
    "    \n",
    "    elif pd.notna(pre_Y_train.loc[index, \"household_income_usa\"]): # USD; HH < 3\n",
    "        income = pre_Y_train.loc[index, \"household_income_usa\"]\n",
    "        if income in ['< $15,000']:\n",
    "            Y_train.at[index, \"SES\"] = 0\n",
    "        elif income in ['$15,000 to $29,999', '$30,000 to $$49,999']:\n",
    "            Y_train.at[index, \"SES\"] = 1\n",
    "        elif income in ['$50,000 to $99,999']:\n",
    "            Y_train.at[index, \"SES\"] = 2\n",
    "        elif income in ['$100,000 to $149,999', '$150,000 to $199,999', '$200,000 to $249,999', '>$250,000']:\n",
    "            Y_train.at[index, \"SES\"] = 3\n",
    "        elif income in ['Prefer not to answer']:\n",
    "            continue\n",
    "        else:\n",
    "            print(income)\n",
    "            raise ValueError(\"Wrong value for household_income_usa\")\n",
    "        \n",
    "    elif pd.notna(pre_Y_train.loc[index, \"household_income_ca\"]):  # CA; HH >= 3\n",
    "        income = pre_Y_train.loc[index, \"household_income_ca\"]\n",
    "        if income in ['< $15,000', '$15,000 to $29,999']:\n",
    "            Y_train.at[index, \"SES\"] = 0\n",
    "        elif income in ['$30,000 to $$49,999']:\n",
    "            Y_train.at[index, \"SES\"] = 1\n",
    "        elif income in ['$50,000 to $99,999', '$100,000 to $149,999']:\n",
    "            Y_train.at[index, \"SES\"] = 2\n",
    "        elif income in ['$150,000 to $199,999', '$200,000 to $249,999', '>$250,000']:\n",
    "            Y_train.at[index, \"SES\"] = 3\n",
    "        elif income in ['Prefer not to answer']:\n",
    "            continue\n",
    "        else:\n",
    "            print(income)\n",
    "            raise ValueError(\"Wrong value for household_income_ca\")\n",
    "        \n",
    "    else:\n",
    "        print(index)\n",
    "        \n",
    "\n",
    "Y_train_encoded = torch.nn.functional.one_hot(torch.tensor(Y_train[\"SES\"].values, dtype=torch.int64))      \n",
    "print(\"Encoded labels: \", Y_train_encoded[0:3])\n",
    "print(\"Shape: \", Y_train_encoded.shape)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audiofiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainbowAudioDataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, data, labels):\n",
    "\t\t# self.segment_size = segment_size\n",
    "\t\tself.data = data\n",
    "\t\tself.labels = labels\n",
    "\t\t\n",
    "\t\t# get location for every recording of rainbow passage\n",
    "\t\tfor index, row in data.iterrows():\n",
    "\t\t\tsubject = \"sub-\"+row['record_id']\n",
    "\t\t\tsession = \"ses-\"+row['demographics_session_id']\n",
    "\t\t\tlocation = str(\"../bids_with_sensitive_recordings/\" + subject + \"/\" + session + '/audio/'+subject+\"_\"+session+\"_Rainbow-Passage_rec-Rainbow-Passage.wav\")\n",
    "\t\t\tif os.path.exists(location):\n",
    "\t\t\t\tself.data.at[index, \"audio_location\"] = location\n",
    "\t\t\telse:\n",
    "\t\t\t\t# print all patients without Rainbow passage recording   \n",
    "\t\t\t\t# print(location)\n",
    "\t\t\t\t# data.at[index, \"audio_location\"] = None\n",
    "\t\t\t\tself.data = self.data.drop(index = index)\n",
    "\t\t\t\tself.labels = torch.cat((self.labels[:index], self.labels[index + 1:]), dim=0)\n",
    "\t\tself.data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\taudio = Audio.from_file(self.data.at[idx, \"audio_location\"])\n",
    "\t\taudio = audio.to_16khz().signal.squeeze()\n",
    "\t\td = (audio.size(0)-282947)//2\n",
    "\t\taudio = audio[d:d+282947]\n",
    "\t\treturn audio, self.label[idx]\n",
    "\n",
    "\t\t# # get middle K seconds if audio is too long, pad with zeros if it is too short\n",
    "\t\t# if audio.size(0) > self.segment_size*16000:\n",
    "\t\t# \td = (audio.size(0)-self.segment_size*16000)//2\n",
    "\t\t# \taudio = audio[d:d+self.segment_size*16000]\n",
    "\t\t# else:\n",
    "\t\t# \taudio = torch.nn.functional.pad(audio, (0,self.segment_size*16000-audio.size(0)), mode='constant', value=0)\n",
    "\n",
    "\tdef analyze_length(self):\n",
    "\t\ttotal_length = 0\n",
    "\t\tmin_val = 10e9\n",
    "\t\tmax_val = 0\n",
    "\t\tfor idx in range(len(self.data)):\n",
    "\t\t\taudio = Audio.from_file(self.data.at[idx, \"audio_location\"])\n",
    "\t\t\taudio = audio.to_16khz().signal.squeeze()\n",
    "\t\t\tlength = audio.size(0)  # Number of samples in the audio\n",
    "\t\t\tmin_val = min_val if min_val < length else length\n",
    "\t\t\tmax_val = max_val if max_val > length else length\n",
    "\t\t\ttotal_length += length\n",
    "\t\taverage_length = total_length / len(self.data)\n",
    "\t\treturn average_length, min_val, max_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([115, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470006.49707602337 282947 2278528\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset[:int(0.8*N)], batch_size=8, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(dataset[int(0.8*N):int(0.9*N)], batch_size=8, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset[int(0.9*N):], batch_size=8, shuffle=False)\n",
    "\n",
    "dataset = RainbowAudioDataset(X_train, Y_train_encoded)\n",
    "avg, min_val, max_val = dataset.analyze_length()\n",
    "print(avg, min_val, max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "slice(None, 136, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n\u001b[1;32m----> 3\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset[\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m\u001b[38;5;241m*\u001b[39mN):\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.9\u001b[39m\u001b[38;5;241m*\u001b[39mN)], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset[\u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.9\u001b[39m\u001b[38;5;241m*\u001b[39mN):], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[23], line 26\u001b[0m, in \u001b[0;36mRainbowAudioDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 26\u001b[0m \taudio \u001b[38;5;241m=\u001b[39m Audio\u001b[38;5;241m.\u001b[39mfrom_file(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio_location\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     27\u001b[0m \taudio \u001b[38;5;241m=\u001b[39m audio\u001b[38;5;241m.\u001b[39mto_16khz()\u001b[38;5;241m.\u001b[39msignal\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     28\u001b[0m \td \u001b[38;5;241m=\u001b[39m (audio\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m282947\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\artkr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:2488\u001b[0m, in \u001b[0;36m_AtIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2485\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid call for scalar access (getting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mloc[key]\n\u001b[1;32m-> 2488\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\artkr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:2440\u001b[0m, in \u001b[0;36m_ScalarAccessIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid call for scalar access (getting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2439\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_key(key)\n\u001b[1;32m-> 2440\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\artkr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:4012\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   4006\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_engine\n\u001b[0;32m   4008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   4009\u001b[0m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[0;32m   4010\u001b[0m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[0;32m   4011\u001b[0m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n\u001b[1;32m-> 4012\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4013\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[row]\n\u001b[0;32m   4015\u001b[0m \u001b[38;5;66;03m# For MultiIndex going through engine effectively restricts us to\u001b[39;00m\n\u001b[0;32m   4016\u001b[0m \u001b[38;5;66;03m#  same-length tuples; see test_get_set_value_no_partial_indexing\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\artkr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\range.py:419\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "File \u001b[1;32mc:\\Users\\artkr\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5974\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5970\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   5971\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[0;32m   5972\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[0;32m   5973\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[1;32m-> 5974\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: slice(None, 136, None)"
     ]
    }
   ],
   "source": [
    "N = len(dataset)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset[:int(0.8*N)], batch_size=8, shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(dataset[int(0.8*N):int(0.9*N)], batch_size=8, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset[int(0.9*N):], batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
